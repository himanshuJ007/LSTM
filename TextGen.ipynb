{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Time\n",
       "15:26:37    Today we express our deepest gratitude to all ...\n",
       "13:33:35    Busy day planned in New York. Will soon be mak...\n",
       "11:14:20    Love the fact that the small groups of protest...\n",
       "2:19:44     Just had a very open and successful presidenti...\n",
       "2:10:46     A fantastic day in D.C. Met with President Oba...\n",
       "Name: Tweet_Text, dtype: object"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = pd.read_csv('C:/Users/hamuj/Desktop/ml/Class/LSTM/Crowdbabble_Social-Media-Analytics_Twitter-Download_Donald-Trump_7375-Tweets.csv', index_col=1, encoding = 'ISO-8859–1')\n",
    "ds.Tweet_Text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Today we express our deepest gratitude to all those who have served in our armed forces. #ThankAVet https://t.co/wPk7QWpK8Z Busy day planned in New York. Will soon be making some very important decisions on the people who will be running our government! Love the fact that the small groups of protesters last night have passion for our great country. We will all come together and be proud! Just had a very open and successful presidential election. Now professional protesters, incited by the media, are protesting. Very unfair! A fantastic day in D.C. Met with President Obama for first time. Really good meeting, great chemistry. Melania liked Mrs. O a lot! Happy 241st birthday to the U.S. Marine Corps! Thank you for your service!! https://t.co/Lz2dhrXzo4 Such a beautiful and important evening! The forgotten man and woman will never be forgotten again. We will all come together as never before Watching the returns at 9:45pm. \n",
      "#ElectionNight #MAGA_Ùà¼_Ùàü https://t.co/HfuJeRZbod RT @IvankaTr\n"
     ]
    }
   ],
   "source": [
    "# Take all the text together\n",
    "\n",
    "data = ' '.join([ix for ix in ds.Tweet_Text])\n",
    "print (data[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{')', 'Î', 'ö', 'm', 'r', '\"', '=', 'y', ':', ',', '\\x99', '\\x8f', 'Ð', 'c', '1', 'B', 'w', 'Ó', 'h', 'Ò', 'Â', '¼', '\\xa0', '¥', 'O', '´', ' ', 'Ô', 'i', '#', '_', 'T', 'K', 'b', 'Y', '5', '÷', 'Z', 'U', '¬', 'â', '|', '2', 'o', 'F', '«', 'Ù', '3', 'j', 'S', '\\x81', '[', 'f', 'z', '?', 'Ñ', 'E', '&', 'X', '~', '8', 'Û', '*', 'ü', 'd', 'e', '»', '4', 'Ä', 'x', '¡', 'R', '@', 'G', 'A', '!', 'Á', '\\n', '%', 'Õ', 'p', 'M', 'I', 'g', '9', '\\x8d', 'À', 'Ü', 'N', 'Ê', 'ª', 'É', '.', 'å', 'ä', '\\x80', '7', 'a', '$', 'C', '-', '(', 'È', 'L', 'à', 'H', '\\x8b', '{', ']', 'ø', '£', 'J', 'Q', '0', '6', 't', 'Ï', '¢', 'Ì', '}', '/', 'u', '\\x9d', '\\x95', 'n', 'W', ';', 'D', '±', 'V', 'l', 'k', '\\x89', 's', 'q', '+', 'v', 'P'}\n",
      "138\n"
     ]
    }
   ],
   "source": [
    "print (set(data))\n",
    "print (len(set(data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[')', 'Î', 'ö', 'm', 'r', '\"', '=', 'y', ':', ',', '\\x99', '\\x8f', 'Ð', 'c', '1', 'B', 'w', 'Ó', 'h', 'Ò', 'Â', '¼', '\\xa0', '¥', 'O', '´', ' ', 'Ô', 'i', '#', '_', 'T', 'K', 'b', 'Y', '5', '÷', 'Z', 'U', '¬', 'â', '|', '2', 'o', 'F', '«', 'Ù', '3', 'j', 'S', '\\x81', '[', 'f', 'z', '?', 'Ñ', 'E', '&', 'X', '~', '8', 'Û', '*', 'ü', 'd', 'e', '»', '4', 'Ä', 'x', '¡', 'R', '@', 'G', 'A', '!', 'Á', '\\n', '%', 'Õ', 'p', 'M', 'I', 'g', '9', '\\x8d', 'À', 'Ü', 'N', 'Ê', 'ª', 'É', '.', 'å', 'ä', '\\x80', '7', 'a', '$', 'C', '-', '(', 'È', 'L', 'à', 'H', '\\x8b', '{', ']', 'ø', '£', 'J', 'Q', '0', '6', 't', 'Ï', '¢', 'Ì', '}', '/', 'u', '\\x9d', '\\x95', 'n', 'W', ';', 'D', '±', 'V', 'l', 'k', '\\x89', 's', 'q', '+', 'v', 'P']\n"
     ]
    }
   ],
   "source": [
    "# Create Vocab\n",
    "vocab = list(set(data))\n",
    "\n",
    "print(vocab)\n",
    "\n",
    "i2c, c2i = {}, {}\n",
    "\n",
    "for idx, chx in enumerate(vocab):\n",
    "    i2c[idx] = chx\n",
    "    c2i[chx] = idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[115  18  28 133  26  28 133  26   3   7  26 133 115   4  28 124  83]]\n"
     ]
    }
   ],
   "source": [
    "def get_onehot(x):\n",
    "    # Take input a string and convert to one-hot encoding\n",
    "    vec_size = len(c2i.keys())\n",
    "    n_seq = len(x)\n",
    "    data = np.zeros((1, n_seq, vec_size))\n",
    "    \n",
    "    # For each element in the list\n",
    "    for ix in range(n_seq):\n",
    "        curr_char = x[ix]\n",
    "        oh_index = c2i[curr_char]\n",
    "        # print ix, curr_char, oh_index\n",
    "        data[:, ix, oh_index] = 1\n",
    "    return data\n",
    "\n",
    "print (get_onehot('this is my string').argmax(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 123, 138)\n",
      "(1, 129, 138)\n",
      "(1, 136, 138)\n",
      "(1, 138, 138)\n",
      "(1, 130, 138)\n",
      "(1, 99, 138)\n",
      "(1, 140, 138)\n",
      "(1, 85, 138)\n",
      "(1, 142, 138)\n",
      "(1, 142, 138)\n"
     ]
    }
   ],
   "source": [
    "for ix in ds.Tweet_Text[:10]:\n",
    "    print (get_onehot(ix).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CharNN(nn.Module):\n",
    "    def __init__(self, in_shape=None, out_shape=None, hidden_shape=None):\n",
    "        super(CharNN, self).__init__()\n",
    "        self.in_shape = in_shape\n",
    "        self.out_shape = out_shape\n",
    "        self.hidden_shape = hidden_shape\n",
    "        self.n_layers = 1\n",
    "        \n",
    "        self.rnn = nn.LSTM(\n",
    "            input_size=self.in_shape,\n",
    "            hidden_size=self.hidden_shape,\n",
    "            num_layers=self.n_layers,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.out = nn.Linear(self.hidden_shape, self.out_shape)\n",
    "    \n",
    "    def forward(self, x, h):\n",
    "        r_out, h_state = self.rnn(x, h)\n",
    "        \n",
    "        outs = []\n",
    "        for ix in range(r_out.size(1)):\n",
    "            current_out = F.softmax(self.out(r_out[:, ix, :]))\n",
    "            outs.append(current_out)\n",
    "        return torch.stack(outs, dim=1), h_state\n",
    "    \n",
    "    def predict(self, char, h=None, top_k=None):\n",
    "        if h is None:\n",
    "            h = self.init_hidden(1, gpu=False)\n",
    "        \n",
    "        x = get_onehot(char)\n",
    "        out, h = self.forward(torch.FloatTensor(x), h)\n",
    "        \n",
    "        p = out.data\n",
    "        if top_k is None:\n",
    "            top_ch = np.arange(self.out_shape)\n",
    "        else:\n",
    "            p, top_ch = p.topk(top_k)\n",
    "            top_ch = top_ch.numpy().squeeze()\n",
    "        \n",
    "        p = p.numpy().squeeze()\n",
    "        char = np.random.choice(top_ch, p=p/p.sum())\n",
    "        return i2c[char], h\n",
    "    \n",
    "    def init_hidden(self, batch_size, gpu=False):\n",
    "        if gpu:\n",
    "            return (Variable(torch.zeros(self.n_layers, batch_size, self.hidden_shape).cuda()),\n",
    "                    Variable(torch.zeros(self.n_layers, batch_size, self.hidden_shape)).cuda())\n",
    "        return (Variable(torch.zeros(self.n_layers, batch_size, self.hidden_shape)),\n",
    "                Variable(torch.zeros(self.n_layers, batch_size, self.hidden_shape)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CharNN(\n",
      "  (rnn): LSTM(138, 256, batch_first=True)\n",
      "  (out): Linear(in_features=256, out_features=138, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = CharNN(in_shape=138, out_shape=138, hidden_shape=256)\n",
    "#model.summary()\n",
    "print (model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/shubham/all_projects/CB/Summer_2018/data/checkpoints/text_gen/model_256h_epoch_38.ckpt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-3e8a5e0f547d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Load the weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/home/shubham/all_projects/CB/Summer_2018/data/checkpoints/text_gen/model_256h_epoch_38.ckpt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module)\u001b[0m\n\u001b[0;32m    299\u001b[0m             \u001b[1;33m(\u001b[0m\u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mversion_info\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m3\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpathlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    300\u001b[0m         \u001b[0mnew_fd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 301\u001b[1;33m         \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    302\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    303\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/shubham/all_projects/CB/Summer_2018/data/checkpoints/text_gen/model_256h_epoch_38.ckpt'"
     ]
    }
   ],
   "source": [
    "# Load the weights\n",
    "model.load_state_dict(torch.load('/home/shubham/all_projects/CB/Summer_2018/data/checkpoints/text_gen/model_256h_epoch_38.ckpt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.predict('a', top_k=20)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "122\n",
      "122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hamuj\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:22: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 4.927226543426514 at Epoch: 0 | Seq: 0\n",
      "128\n",
      "128\n",
      "135\n",
      "135\n",
      "137\n",
      "137\n",
      "129\n",
      "129\n",
      "98\n",
      "98\n",
      "139\n",
      "139\n",
      "84\n",
      "84\n",
      "141\n",
      "141\n",
      "141\n",
      "141\n",
      "141\n",
      "141\n",
      "72\n",
      "72\n",
      "129\n",
      "129\n",
      "96\n",
      "96\n",
      "111\n",
      "111\n",
      "59\n",
      "59\n",
      "139\n",
      "139\n",
      "140\n",
      "140\n",
      "33\n",
      "33\n",
      "139\n",
      "139\n",
      "79\n",
      "79\n",
      "91\n",
      "91\n",
      "136\n",
      "136\n",
      "136\n",
      "136\n",
      "77\n",
      "77\n",
      "138\n",
      "138\n",
      "141\n",
      "141\n",
      "127\n",
      "127\n",
      "143\n",
      "143\n",
      "120\n",
      "120\n",
      "83\n",
      "83\n",
      "32\n",
      "32\n",
      "141\n",
      "141\n",
      "33\n",
      "33\n",
      "134\n",
      "134\n",
      "135\n",
      "135\n",
      "134\n",
      "134\n",
      "134\n",
      "134\n",
      "140\n",
      "140\n",
      "144\n",
      "144\n",
      "125\n",
      "125\n",
      "136\n",
      "136\n",
      "97\n",
      "97\n",
      "73\n",
      "73\n",
      "102\n",
      "102\n",
      "139\n",
      "139\n",
      "147\n",
      "147\n",
      "136\n",
      "136\n",
      "76\n",
      "76\n",
      "147\n",
      "147\n",
      "38\n",
      "38\n",
      "86\n",
      "86\n",
      "111\n",
      "111\n",
      "111\n",
      "111\n",
      "139\n",
      "139\n",
      "132\n",
      "132\n",
      "139\n",
      "139\n",
      "24\n",
      "24\n",
      "149\n",
      "149\n",
      "79\n",
      "79\n",
      "116\n",
      "116\n",
      "139\n",
      "139\n",
      "140\n",
      "140\n",
      "56\n",
      "56\n",
      "135\n",
      "135\n",
      "127\n",
      "127\n",
      "145\n",
      "145\n",
      "85\n",
      "85\n",
      "144\n",
      "144\n",
      "77\n",
      "77\n",
      "123\n",
      "123\n",
      "141\n",
      "141\n",
      "145\n",
      "145\n",
      "133\n",
      "133\n",
      "57\n",
      "57\n",
      "141\n",
      "141\n",
      "139\n",
      "139\n",
      "143\n",
      "143\n",
      "136\n",
      "136\n",
      "130\n",
      "130\n",
      "111\n",
      "111\n",
      "141\n",
      "141\n",
      "52\n",
      "52\n",
      "141\n",
      "141\n",
      "116\n",
      "116\n",
      "127\n",
      "127\n",
      "135\n",
      "135\n",
      "141\n",
      "141\n",
      "138\n",
      "138\n",
      "111\n",
      "111\n",
      "124\n",
      "124\n",
      "139\n",
      "139\n",
      "136\n",
      "136\n",
      "137\n",
      "137\n",
      "147\n",
      "147\n",
      "142\n",
      "142\n",
      "145\n",
      "145\n",
      "115\n",
      "115\n",
      "105\n",
      "105\n",
      "132\n",
      "132\n",
      "104\n",
      "104\n",
      "74\n",
      "74\n",
      "121\n",
      "121\n",
      "83\n",
      "83\n",
      "125\n",
      "125\n",
      "137\n",
      "137\n",
      "147\n",
      "147\n",
      "138\n",
      "138\n",
      "130\n",
      "130\n",
      "133\n",
      "133\n",
      "128\n",
      "128\n",
      "54\n",
      "54\n",
      "95\n",
      "95\n",
      "140\n",
      "140\n",
      "140\n",
      "140\n",
      "124\n",
      "124\n",
      "139\n",
      "139\n",
      "142\n",
      "142\n",
      "138\n",
      "138\n",
      "140\n",
      "140\n",
      "143\n",
      "143\n",
      "141\n",
      "141\n",
      "109\n",
      "109\n",
      "138\n",
      "138\n",
      "137\n",
      "137\n",
      "134\n",
      "134\n",
      "140\n",
      "140\n",
      "90\n",
      "90\n",
      "137\n",
      "137\n",
      "37\n",
      "37\n",
      "139\n",
      "139\n",
      "133\n",
      "133\n",
      "75\n",
      "75\n",
      "134\n",
      "134\n",
      "131\n",
      "131\n",
      "111\n",
      "111\n",
      "35\n",
      "35\n",
      "116\n",
      "116\n",
      "109\n",
      "109\n",
      "75\n",
      "75\n",
      "79\n",
      "79\n",
      "142\n",
      "142\n",
      "133\n",
      "133\n",
      "139\n",
      "139\n",
      "69\n",
      "69\n",
      "141\n",
      "141\n",
      "142\n",
      "142\n",
      "122\n",
      "122\n",
      "134\n",
      "134\n",
      "66\n",
      "66\n",
      "145\n",
      "145\n",
      "88\n",
      "88\n",
      "102\n",
      "102\n",
      "145\n",
      "145\n",
      "130\n",
      "130\n",
      "57\n",
      "57\n",
      "135\n",
      "135\n",
      "138\n",
      "138\n",
      "142\n",
      "142\n",
      "112\n",
      "112\n",
      "98\n",
      "98\n",
      "130\n",
      "130\n",
      "118\n",
      "118\n",
      "146\n",
      "146\n",
      "151\n",
      "151\n",
      "137\n",
      "137\n",
      "140\n",
      "140\n",
      "140\n",
      "140\n",
      "141\n",
      "141\n",
      "28\n",
      "28\n",
      "133\n",
      "133\n",
      "129\n",
      "129\n",
      "44\n",
      "44\n",
      "135\n",
      "135\n",
      "116\n",
      "116\n",
      "111\n",
      "111\n",
      "117\n",
      "117\n",
      "139\n",
      "139\n",
      "133\n",
      "133\n",
      "37\n",
      "37\n",
      "93\n",
      "93\n",
      "37\n",
      "37\n",
      "37\n",
      "37\n",
      "116\n",
      "116\n",
      "68\n",
      "68\n",
      "141\n",
      "141\n",
      "79\n",
      "79\n",
      "140\n",
      "140\n",
      "139\n",
      "139\n",
      "138\n",
      "138\n",
      "89\n",
      "89\n",
      "134\n",
      "134\n",
      "137\n",
      "137\n",
      "140\n",
      "140\n",
      "137\n",
      "137\n",
      "145\n",
      "145\n",
      "95\n",
      "95\n",
      "141\n",
      "141\n",
      "111\n",
      "111\n",
      "143\n",
      "143\n",
      "122\n",
      "122\n",
      "127\n",
      "127\n",
      "139\n",
      "139\n",
      "108\n",
      "108\n",
      "133\n",
      "133\n",
      "75\n",
      "75\n",
      "115\n",
      "115\n",
      "141\n",
      "141\n",
      "139\n",
      "139\n",
      "49\n",
      "49\n",
      "106\n",
      "106\n",
      "140\n",
      "140\n",
      "120\n",
      "120\n",
      "116\n",
      "116\n",
      "132\n",
      "132\n",
      "131\n",
      "131\n",
      "145\n",
      "145\n",
      "83\n",
      "83\n",
      "80\n",
      "80\n",
      "88\n",
      "88\n",
      "102\n",
      "102\n",
      "91\n",
      "91\n",
      "127\n",
      "127\n",
      "146\n",
      "146\n",
      "140\n",
      "140\n",
      "84\n",
      "84\n",
      "96\n",
      "96\n",
      "73\n",
      "73\n",
      "141\n",
      "141\n",
      "139\n",
      "139\n",
      "141\n",
      "141\n",
      "137\n",
      "137\n",
      "130\n",
      "130\n",
      "132\n",
      "132\n",
      "127\n",
      "127\n",
      "144\n",
      "144\n",
      "141\n",
      "141\n",
      "143\n",
      "143\n",
      "129\n",
      "129\n",
      "127\n",
      "127\n",
      "135\n",
      "135\n",
      "139\n",
      "139\n",
      "145\n",
      "145\n",
      "131\n",
      "131\n",
      "138\n",
      "138\n",
      "141\n",
      "141\n",
      "114\n",
      "114\n",
      "139\n",
      "139\n",
      "78\n",
      "78\n",
      "141\n",
      "141\n",
      "140\n",
      "140\n",
      "124\n",
      "124\n",
      "135\n",
      "135\n",
      "140\n",
      "140\n",
      "49\n",
      "49\n",
      "138\n",
      "138\n",
      "79\n",
      "79\n",
      "117\n",
      "117\n",
      "135\n",
      "135\n",
      "131\n",
      "131\n",
      "132\n",
      "132\n",
      "109\n",
      "109\n",
      "134\n",
      "134\n",
      "123\n",
      "123\n",
      "131\n",
      "131\n",
      "74\n",
      "74\n",
      "116\n",
      "116\n",
      "73\n",
      "73\n",
      "104\n",
      "104\n",
      "131\n",
      "131\n",
      "117\n",
      "117\n",
      "98\n",
      "98\n",
      "75\n",
      "75\n",
      "141\n",
      "141\n",
      "114\n",
      "114\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-4ccc256133de>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[1;31m# optimizer.zero_grad()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[1;31m# gradient clipping to solve exploding/vanishing grads\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m         \"\"\"\n\u001b[1;32m---> 93\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     87\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m     88\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 89\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m     90\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Set to train mode\n",
    "# model.cuda()\n",
    "model.train()\n",
    "N = 5000\n",
    "\n",
    "for epoch in range(2):\n",
    "    total_loss = 0\n",
    "    # For each sequence\n",
    "    for qx in range(N):\n",
    "        seqx = ds.Tweet_Text[qx]\n",
    "        #print(seqx)\n",
    "        print(len(seqx[:-1]))\n",
    "        print(len(seqx[1:]))\n",
    "        h_state = model.init_hidden(1)\n",
    "        input_seq = seqx[:-1]\n",
    "        target_seq = seqx[1:]\n",
    "        \n",
    "        x = Variable(torch.FloatTensor(get_onehot(input_seq)), requires_grad=True)# .cuda()\n",
    "        y = Variable(torch.LongTensor(get_onehot(target_seq).argmax(2)))# .cuda()\n",
    "        \n",
    "        model.zero_grad()\n",
    "        pred, h_state = model.forward(x, h_state)\n",
    "        # print pred.squeeze().shape, y.shape\n",
    "        loss = criterion(pred.squeeze(), y.squeeze())\n",
    "        \n",
    "        # optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # gradient clipping to solve exploding/vanishing grads\n",
    "        # clip = 5.0\n",
    "        # nn.utils.clip_grad_norm(net.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        total_loss += loss\n",
    "        if qx%(N/5) == 0:\n",
    "            print ('Loss: {} at Epoch: {} | Seq: {}'.format(loss, epoch, qx))\n",
    "        \n",
    "    print (\"OverflowErrorerflowErrorall Average Loss: {} at Epoch: {}\".format(total_loss / float(N), epoch))\n",
    "    \n",
    "    # Save model checkpoints\n",
    "    if epoch % 10 == 0:\n",
    "        torch.save(model.state_dict(), \"./data/checkpoints/text_gen/model_256h_epoch_{}.ckpt\".format(epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:22: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "or @Markeressoness @Markeresson @realDonaldTrump @Markething and an amappathing and an amarian and an amasting and an amarinate and an amater and an amarinate and an amas and an amasting and an amater and an amapart and an amarian and an amarian and an amarical and an amas and an amarinate and an amas and an amarian and an amas and an amasting and an amarian and an amarian and an amarian and an amasting and an amas and an amas and an amas and an amater and an amas and an amappare and an amas and an amarical and an amas and an amappard in an amaring and an amater and an amappare and an amas and an amarian and an amarinate and an amasting and an aman and an amater and an amas and an amater and an amarical and an amarian and an amarical and an amas and an amas and an amas and an amasting and an amarical and an amas and an amas and an amaricated and an amas and an amarian and an amas and an amater and an amas and an amas and an amasting and an amas and an amater and an amas and an amater an\n"
     ]
    }
   ],
   "source": [
    "sentence = 'o'\n",
    "model.cpu()\n",
    "h_s = model.init_hidden(1, gpu=False)\n",
    "for ix in range(1000):\n",
    "    char = sentence[-1]\n",
    "    out, h = model.predict(char, h=h_s, top_k=100)\n",
    "    h_s = (h[0].data, h[1].data)\n",
    "    \n",
    "    sentence += out\n",
    "print sentence"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
